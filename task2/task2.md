# Task 2

## GBDT 算法梳理
GBDT是陈天奇大佬提出来的，是一种加法模型（即基函数的线性组合）与前向分布算法并以决策树作为基函数的提升方法，想比较与随机森林，GBDT偏向于降低模型的误差，通过按序训练的方式来生成，相比较与随机森林的训练来说比较慢。 

首先了解一下提升树，提升树式以分类树或回归树为基本分类器的提升方法，提升树被认为是统计学习中性能最好的方法之一。 

提升方法实际采用加法模型（即奇函数的线性组合）与前向分布算法，以决策树为基函数的提升方法称为提升树，对于分类问题决策树是二叉分类树

1. 前向分布算法
    GBDT是一个加法模型，最终的目的构建决策树的线性组合来得到最终的分类器，首先看一下加法模型:$f(x)= \sum_{m=1}^M\beta_{m}b(x;\gamma_{m}).$ \
    其中$b(x;\gamma_{m})$是基分类器，$beta_{m}$是对应第$m$个分类器的权重系数，即该基分类器在最后的分类器中的权重系数 \
    在给定其损失函数 $L(y,f(x))$，学习我们所需要的加法模型使得该损失函数最小，我们可以得到目标函数：
    $$ min_{\beta_{m},\gamma_{m}}\sum_{n=1}^NL(y,\sum_{m=1}^M\beta_{m}b(x;\gamma_{m}))$$      
    

    前向分布算法，每一步我们只需学习一个基函数（基分类器使得目标函数最小），然后在原先学习的模型中加入继续往后学习，所以每一步中我需要学习的是使得一下目标函数变小：
    $$ min_{\beta_{m},\gamma_{m}}\sum_{n=1}^NL(y,\beta_{m}b(x;\gamma_{m}))$$

    注意上面这个公式是前向分布的每一步的优化目标

2. 负梯度拟合

3. 损失函数

4. 回归

5. 二分类，多酚类

6. 正则化

7. 优缺点

8. sklearn参数

9. 应用场景

参考资料: \
    西瓜书 \
    cs229吴恩达机器学习课程 \
    李航统计学习 \
    谷歌搜索 \
    公式推导参考：http://t.cn/EJ4F9Q0
